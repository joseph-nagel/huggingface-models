{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57563e58",
   "metadata": {},
   "source": [
    "# transformers: OWL-ViT for object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd4b64",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e975f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.imshow(np.asarray(image))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230510c",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name\n",
    "model_name = 'google/owlvit-base-patch32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84be8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text and image processors\n",
    "processor = OwlViTProcessor.from_pretrained(model_name)\n",
    "\n",
    "# load model\n",
    "model = OwlViTForObjectDetection.from_pretrained(model_name, device_map='auto')\n",
    "model = model.eval()\n",
    "\n",
    "print(f'Model device: {model.device}')\n",
    "print(f'Model dtype: {model.dtype}')\n",
    "print(f'Memory footprint: {model.get_memory_footprint() * 1e-9:.2f} GiB')\n",
    "\n",
    "print(f'\\nEmbedding dim.: {model.config.projection_dim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f62fe7",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set candidate labels\n",
    "candidate_labels = ['cat', 'dog', 'car', 'remote', 'blanket']\n",
    "\n",
    "# preprocess inputs\n",
    "inputs = processor(\n",
    "    text=candidate_labels,\n",
    "    images=image,\n",
    "    return_tensors='pt',\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(f'Input IDs shape: {inputs['input_ids'].shape}')\n",
    "print(f'Pixel values shape: {inputs['pixel_values'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs.to(model.device))\n",
    "\n",
    "bboxes = outputs.pred_boxes.cpu() # (batch_size, num_boxes, 4)\n",
    "logits = outputs.logits.cpu() # (batch_size, num_boxes, num_labels)\n",
    "probs = logits.softmax(dim=-1)\n",
    "\n",
    "print(f'Bounding boxes shape: {bboxes.shape}') # bboxes in Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "print(f'Logits shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess outputs\n",
    "detections = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    threshold=0.1,\n",
    "    target_sizes=[(image.height, image.width)],\n",
    "    text_labels=[candidate_labels]\n",
    ")\n",
    "\n",
    "# summarize detections\n",
    "for label, score, bbox in zip(\n",
    "    detections[0]['text_labels'],\n",
    "    detections[0]['scores'],\n",
    "    detections[0]['boxes']\n",
    "):\n",
    "    box = [round(coord, 2) for coord in bbox.tolist()]\n",
    "    print(f'{label} ({score:.2f}) in {box}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colors for bounding boxes\n",
    "colors = ['green', 'orange', 'purple', 'green', 'red']\n",
    "unique_label_ids = torch.unique(detections[0]['labels']).tolist()\n",
    "color_dict = {lidx: colors[idx % len(colors)] for idx, lidx in enumerate(unique_label_ids)}\n",
    "\n",
    "# add bounding boxes to the image\n",
    "image_tensor = torch.as_tensor(np.array(image)) # (H, W, C)\n",
    "\n",
    "image_tensor = draw_bounding_boxes(\n",
    "    image_tensor.permute(2, 0, 1), # (C, H, W)\n",
    "    boxes=detections[0]['boxes'],\n",
    "    labels=detections[0]['text_labels'],\n",
    "    colors=[color_dict[lidx] for lidx in detections[0]['labels'].tolist()]\n",
    ").permute(1, 2, 0) # (H, W, C)\n",
    "\n",
    "# show predictions\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.imshow(image_tensor.numpy())\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title(f'Predictions')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
